%%%% kr-instructions.tex -- version 1.3 (11-Jan-2021)

\typeout{KR2025 Instructions for Authors}

% These are the instructions for authors for KR-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{kr}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{acronym}
\usepackage{natbib}
\usepackage{xspace}
\usepackage{xcolor}
\urlstyle{same} 

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{observation}{Observation}








\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\params}{\textit{params}}
\newcommand{\actions}{\textit{actions}}
\newcommand{\objects}{\textit{objects}}
\newcommand{\lifted}{\textit{lifted}}
\newcommand{\relevant}{\textit{relevant}}


\newcommand{\actionname}{\textit{name}}
\newcommand{\type}{\textit{type}}
\newcommand{\cnf}{\textit{CNF}}
\newcommand{\conj}{\textit{Conj}}
\newcommand{\realm}{\ensuremath{M^*}\xspace}
\newcommand{\liftf}{\mathsf{f}}
% \newcommand{\liftl}{\mathsf{l}} RONI: This looks too much like "one"
\newcommand{\liftl}{\ensuremath{\ell}} 
\newcommand{\lifta}{\mathsf{\alpha}}
\newcommand{\liftatag}{\mathsf{\alpha}'}
\newcommand{\jointa}{\hat{a}}
\newcommand{\esam}{\textit{ESAM}\xspace}
\newcommand{\sgam}{\textit{SGAM}\xspace}
\newcommand{\bindings}{\textit{bindings}}
\newcommand{\iseff}{\text{IsEff}}
\newcommand{\ispre}{\text{IsPre}}
\newcommand{\domain}{\textit{Dom}}


\newcommand{\pre}{\ensuremath{\textit{pre}}\xspace}
\newcommand{\eff}{\ensuremath{\textit{eff}}\xspace}
\newcommand{\app}{\ensuremath{\textit{app}}\xspace}

\newcommand{\psynpre}{\ensuremath{P^\pre}\xspace}
\newcommand{\psyneff}{\ensuremath{P^\eff}\xspace}
\newcommand{\psempre}{\ensuremath{P^{sem}_\pre}\xspace}
\newcommand{\psemeff}{\ensuremath{P^{sem}_\eff}\xspace}
\newcommand{\rsynpre}{\ensuremath{R^\pre}\xspace}
\newcommand{\rsyneff}{\ensuremath{R^\eff}\xspace}
\newcommand{\rsempre}{\ensuremath{App}\xspace}
\newcommand{\rsemeff}{\ensuremath{R^{sem}_\eff}\xspace}



\acrodef{NO-OP}{No Operation}
\acrodef{CMAP-BB}{CMAP with Black-Box Agents}
\acrodef{MF-MAP}{Model-Free Multi-Agent Planning}
\acrodef{MA-SAM}{Multi-Agent Safe Action Model Learning}
\acrodef{SAM}{Safe Action Model Learning}
\acrodef{JAT}{Joint Action Trajectory}
\acrodef{LMA}{Lifted Macro Action}
% \newcommand{\noop}{\ac{NO-OP}\xspace}
\newcommand{\noop}{\textit{NO-OP}\xspace}
\newcommand{\sam}{\ac{SAM}\xspace}
\newcommand{\masam}{\ac{MA-SAM}\xspace}
\newcommand{\cmasam}{\text{MA-SAM\ensuremath{^+}}\xspace}
% \newcommand{\mfmap}{\ac{MF-MAP}\xspace}
\newcommand{\jat}{\ac{JAT}\xspace}
\newcommand{\blmaa}{\ac{LMA}\xspace}
\newcommand{\blmaas}{LMAs\xspace}
\newcommand{\pbl}{pb-literal\xspace}
\newcommand{\pbls}{pb-literals\xspace}
\newcommand{\learnblmaa}{Learn\blmaa}

\newcommand{\todo}[1]{{\textcolor{red}{[TODO: #1]}}}
\newcommand{\roni}[1]{{\textcolor{red}{[Roni: #1]}}}
\newcommand{\argaman}[1]{{\textcolor{blue}{[Argaman: #1]}}}
\newcommand{\omer}[1]{{\textcolor{purple}{[Omer: #1]}}}
\newcommand{\mauro}[1]{{\textcolor{green}{[Mauro: #1]}}}
\newcommand{\yarin}[1]{{\textcolor{teal}{[Yarin: #1]}}}
\newcommand{\gregor}[1]{{\textcolor{orange}{[Gregor: #1]}}}
\newcommand{\cm}[1]{{\textcolor{olive}{[Christian: #1]}}}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.
%PDF Info Is REQUIRED.
\pdfinfo{
/TemplateVersion (KR.2022.0, KR.2023.0, KR.2024.0, KR.2025.0)
}



\title{Evaluating Planning Model Learning Algorithms \\ Across Different Representations}
\author{Anonymous Authors}

% % Single author syntax
% \iffalse % (remove the multiple-author syntax below and \iffalse ... \fi here)
% \author{%
%     Author name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com    % email
% }
% \fi
% % Multiple author syntax
% \author{%
% First Author$^1$\and
% Second Author$^2$\and
% Third Author$^{2,3}$\and
% Fourth Author$^4$ \\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation \\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% }

\begin{document}

\maketitle

\begin{abstract}
Automated planning is a prominent approach to sequential decision-making. A crucial aspect of domain-independent planning is the domain model, which provides a planning engine with the necessary application knowledge needed to synthesize a plan. A domain model typically includes a state representation and an action model, which defines the set of possible actions and the preconditions and effects of each action. 
Formulating domain models is a challenging, time-consuming, and error-prone task. For this reason, a number of approaches have been proposed to automatically learn complete (or partial) domain models from a set of provided observations. This raises the question of how to compare models learned by different approaches; there is a lack of evaluation metrics, and the complexity is exacerbated since learning algorithms may output models with different representations of states and actions. 
To foster the use of model learning approaches, we describe a set of metrics designed to assess different characteristics of models to be compared. Further, to bridge the potential representation gap between different learned models, we propose an encoder-decoder mechanism that allows for mapping two models regardless of their encoding and demonstrate how this mechanism can be leveraged to systematically compare models using the proposed metrics. Finally, we suggest a benchmark suite based on existing models from the International Planning Competition (IPC) and an evaluation process for using it. 
\end{abstract}
%% outline:

\section{Introduction}

% \roni{TODO: What is automated planning. Focus: classical planning. }
Domain-independent planning is a foundational area of research in Artificial Intelligence (AI) that focuses on the automatic generation of plans to achieve specific goals from a given initial state in a given environment. Classical planning, which is the focus of this work, is the colloquial name for a well-studied type of domain-independent planning problem in which a single agent is acting in a fully observable, discrete, and deterministic environment. 
% \roni{TODO: Creating an action model is hard. Learning action models algorithms exists.}
Most research on classical planning has focused on developing efficient algorithms for solving planning problems, and assumed the existence of a \emph{domain model} specified in a formal language such as the Planning Domain Definition Language (PDDL)~\citep{haslum2019introduction}. 
The domain model in classical planning defines how states are represented, the set of possible actions, and the preconditions and effects of each action. 
However, creating a domain model is a challenging, time-consuming, and error-prone task \citep{DBLP:conf/kcap/McCluskeyVV17}.
This is a bottleneck for the wider dissemination of planning technology in real-world applications. 

% \roni{TODO: Learning action models output action models that use different representations of states and actions.}
To address this issue, a number of algorithms have been proposed to automatically learn domain models from a set of provided observations~\citep{g√∂sgens2025learningfromonlyactiontraces,LAMANNA2025104256,xi2024neuro,mordoch2024safe,juba2021safe,lamanna2021online,cresswell2011generalised,ZHUO20101540}.\roni{Everyone is invited to add their action model learning algorithm here.}
\cm{Why not just point to some of the survey stuff? On the MACQ website (https://macq.planning.domains/), we have the bib entry and pointer to 3 previous survey papers. I'm biased, but I think MACQ should be included :P.}
\roni{Good idea, and I definitely think MACQ should be included. This is a mistake on my end, but will be remedied}
% \roni{TODO: Key question: how to evaluate action model learning algorithms?}
Despite a recent resurgence in interest in learning domain models, there is no standard evaluation process for such algorithms, 
no set of agreed-upon evaluation metrics, and no standard benchmark. 
This paper aims to close this gap and propose an evaluation paradigm for domain model learning algorithms, a set of metrics, and a publicly available benchmark for evaluation. 


We begin our exposition by describing a straightforward evaluation process for action model learning algorithms, which is based on comparing the \emph{syntactic similarity} of the learned domain model to a \emph{reference domain model}. We discuss the limitations of this evaluation method and propose an alternative evaluation process that aims to evaluate the \emph{predictive power} and \emph{problem-solving} ability of the learned domain model. 
Several specific evaluation metrics are defined for this type of evaluation based on prior works~\citep{aineto2019learning,juba2021safe,mordoch2024safe,Oswald2024DLLMDomainModeling}, and we discuss their strengths and weaknesses. 


% \roni{TODO: Examples: grounded vs. lifted, different action names, different object types, paramter ordering, etc.}
The proposed metrics are designed for evaluating models that use the same representation of states and actions. 
However, in practice, action model learning algorithms may output models that use different representations of states and actions. 
For example, some algorithms output models in a grounded representation~\citep{stern2017efficient}, while others output models in a lifted representation~\citep{juba2021safe,xi2024neuro,aineto2019learning}. 
Some represent states based only the parameters of executed actions~\citep{cresswell2011generalised}, or different languages to support human engineers \citep{mccluskey2010action}. 
Finally, some require a richer symbolic representation of states~\citep{juba2021safe}, while others use a visual representation of states~\citep{xi2024neuro}.
\roni{All: add references and refine the above and add more examples of representation gap.}
We propose an evaluation paradigm that addresses this representation gap challenge. 
In this paradigm, an evaluated domain model learning algorithm is obliged to define encoder and decoder functions to bridge the representation gap. 
We adapt the domain model evaluation metrics defined above to use these bridging functions, 
and show to define such functions for several domain model learning algorithms. 
\roni{TODO: Benchmarks, evaluation methodology}
Finally, we describe a benchmark suite and suggest an evaluation process for using it based on existing domain models from the International Planning Competition (IPC). 




\section{Background}

% In this section, we provide the necessary background on classical planning, approaches to automatically learn domain models, and a discussion on domain models comparison. 
% \subsection{Classical Planning}
We focus on \emph{classical planning} problems, which is a well-studied type of planning problem in which a single agent is acting in a fully observable, discrete, and deterministic environment. 
A classical planning problem is defined as a tuple $\mathcal{P} = \tuple{F, A, s_0, G}$, where $F$ is a finite set of fluents, $A$ is a finite set of actions, $s_0 \in S$ is the initial state, and $G\subset F$ is a set of fluents. \roni{For all: I modified the above definition to:(1) not have $S$ as the set of states but have $F$ as the set of fluents, and (2) not have $G$ as a set of states but as a subset of fluents (that must be achieved). If anyone objects please do not change but rather comment here on the change you would like and why.}
A \emph{state} is defined by a set of propositions, representing that the conjunction of fluents in this set are true in this state.\roni{Makes sense? modifications suggested for the sentence above?}
An action $a\in A$ is defined as a tuple $a = \tuple{\mathit{pre}(a), \mathit{eff}(a)}$, where $\mathit{pre}(a)$ is the precondition of $a$ and $\mathit{eff}(a)$ is the effect of $a$. 
The precondition $\mathit{pre}(a)$ specifies the conditions that must hold in a state for the action $a$ to be applicable, while the effect $\mathit{eff}(a)$ specifies the changes to the state resulting from applying the action $a$.   
The precondition and effect of an action are defined as a set of literals, which are either positive or negative propositions. We denote by $L$ as the set of literals, i.e., $L=\{\ell | \exists f\in F: \ell=f \vee \ell=\neg f\}$. \roni{Not sure about the }
A solution to a classical planning problem is a sequence of actions that transforms the initial state $s_0$ into a goal state $s_g$ where $G\subseteq s_g$. \roni{Minor modification to goal state to reflect the change above}


% Discussion on lifted domain representation
Planning domains are usually represented in a lifted representation, where the actions are defined with respect to a set of object types.
The lifted representation of a planning domain is defined as a tuple $\mathcal{D} = \tuple{O, P, A}$, where $O$ is a set of object types, $P$ is a set of predicates, and $A$ is a set of actions. Actions and predicates are parameterized by objects, and preconditions and effects are defined accordingly. 
Popular classical planning systems, such FastDownward~\citep{helmert2006fast}, support such a lifted representation. 

% \subsection{Domain Learning Algorithms}
% Action model learning algorithms
Different algorithms have been proposed for learning domains for classical planning. 
The input to these algorithms is a set of \emph{trajectories}. 
A \emph{trajectory} is a sequence of observations and actions. 
An \emph{observation} can be a state or some other information about the state, 
such as a set of predicates that hold in the state or a visual representation of a state. 
Domain model learning algorithms differ in the type of trajectories they can learn from, the type of action models they can learn, and the representation of the learned action model. 

% Example of prominent domain learning algorithms. 
% \todo{Open request for all: refine and add more algorithms to the following paragraphs as needed. Do not be shy: add your algorithms here.}
%\paragraph{}

\cm{I'm actually not sure any of this is needed. We're probably pushing over 50 related works in the area (cf. MACQ or similar surveys), and there's no way we'll cover the full space of approaches. A few exemplary ones, and then a pointer to a survey should be fine. Unless we need the background of how some of these work, it's about half-a-page of space we could save. Now the \textit{metrics} used in those papers, is something related.}
\roni{I agree this needs to be shortened. TODO for me. Would be very good to have a list of the metrics used in each paper. Great idea. TODO by someone? a table or so with metric and list of papers using it?}

The ARMS algorithm~\citep{yang2007learning} requires as observations the initial and final state of each of the provided trajectories, with optional use of intermediate states if available. For each observed action, ARMS lifts the action and constructs a set of constraints on the fluents involved in its preconditions and effects. These constraints are then resolved using a weighted MAX-SAT solver to generate the action model with the highest weight.
The Simultaneous Learning and Filtering (SLAF)~\citep{amir2008learning} learns lifted action models from trajectories even if some of the states in them are not observed. 
SLAF uses logical inference to filter inconsistent action models and requires observing all actions in the trajectory.  
FAMA~\citep{aineto2019learning} can also handle missing observations and outputs a lifted planning domain model. 
It frames the task of learning an action model as a planning problem, ensuring that the returned action model is consistent with the provided observations.
NOLAM~\citep{Lamanna24} can learn lifted action models even from noisy trajectories. 
LOCM~\citep{cresswell2011generalised} and LOCM2~\citep{cresswell2013acquiring}, and its extension to learn action costs \citep{gregory2016domain} analyze only the sequences of actions in the given trajectories, ignoring any information about the states between them. 
Based on less structured input knowledge, Framer \citep{lindsay2017framer} is an approach for learning planning domain models from natural language descriptions of activity sequences.



% None of the presented algorithms provide execution soundness guarantees, i.e., that plans created with the learned action model are applicable in the real action model. 
The \sam learning algorithms~\citep{stern2017efficient,mordoch2023learning,juba2021safe,juba2022learning,le2024learning,mordoch2024safe} are a family of action model learning algorithms that return action models that guarantee that plans generated with them are applicable in the actual action model. 
An action model having this property is referred to as \emph{safe action model}.
% This property is commonly referred to as a \emph{safety} guarantee.
The \sam family of algorithms addresses learning safe action models under different settings: 
the action models are lifted~\citep{juba2021safe}, with stochastic~\citep{juba2022learning} or conditional effects~\citep{mordoch2024safe}, and even action models containing numeric preconditions and effects~\citep{mordoch2023learning}.
\citet{le2024learning} extended their approach to support learning safe action models in a partially observable environment. 
ESAM~\citep{juba2021safe} is an extension of the \sam algorithm that learns action models from domains in which there is ambiguity regarding the mapping of objects to parameters. To resolve this ambiguity, ESAM outputs a model with additional \emph{proxy actions} that impose additional preconditions and parameter changes on actions in which such ambiguity cannot be resolved. 



LatPlan~\citep{asai2018classical} and ROSAME-I~\citep{xi2024neuro} are conceptually different since they learn propositional action models from trajectories where the states in the given trajectories are given as images, as opposed to a conjunction of fluents. 
\yarin{Rewrote and added part on LatPlan here:}
% LatPlan uses a variational autoencoder with the Gumbel-Softmax trick~\citep{jang2017categorical} to convert image states into discrete propositional symbols, enabling classical planning in latent space.
LatPlan is a fully unsupervised system that uses a variational autoencoder as a differentiable approximation to convert image states into discrete propositional symbols, enabling classical planning in a learned latent space.
% ROSAME-I~\citep{xi2024neuro} is a recent algorithm that learns action models from trajectories where the states are given as images.
% ROSAME-I, in contrast, propose a neural network architecture that learns the action model in a lifted representation in tandem with a neural network that learns to map images to a given symbolic representation. 
ROSAME-I, in contrast, assumes a predefined set of propositions and action signatures. It simultaneously trains a classifier to identify propositions from images and learns a lifted, first-order action model over the given symbolic vocabulary.
% While both approaches learn from image-based trajectories, LatPlan is fully unsupervised, whereas ROSAME-I incorporates prior symbolic knowledge to learn structured models.






\section{Problem Setting and Syntactic Similarity}
\label{sec:problem-setting}

%\mauro{reshaped the section. Please check.}

In this work, we consider the following domain model learning setup. 
\gregor{Here it seems that $\realm$ is not yet a planning model, but a more general ``some environment''. Maybe we should distinguish between the ``environment'' (i.e. the thing that exists in reality) and the planning domain that it can be represented with? This might allow us to talk more concisely about @Mauro's concerns w.r.t.\ the ability to even know this ground truth model.\\
E.g.\ say: An agent is acting in an environment $E$, 
for which we assume that it can be represented as a classical planning domain $\realm$. }
\roni{I like it. Implementing this change below}
An agent is acting in an environment $E$ that we assume can be represented as a classical planning domain denoted by $\realm$. 
The agent's actions are recorded in a set of trajectories, which are sequences of observations and actions. 
The observations and actions in the trajectories are given in a specific representation, which we refer to as the \emph{input representation}. 
A domain model learning algorithm is given this set of trajectories, and is expected to output a domain model in classical planning. 
That is, this domain model can be given as input to a classical planner, and together with an appropriate problem description, the planner can generate plans with it.
\gregor{Maybe add: These plans then should be applicable in the environment $\realm$ and should lead to the goal set in the problem description.}\roni{I intentionally did not add this, as it is not always the case, and we will talk later about cases where we need a ``decoder'' to translate the plans generated by the planner and the thing that can be executed by the agent in the environment.}

%\mauro{clarify here the ground truth model, and discuss issues and alternatives.}
%\roni{Note: changing \emph{real domain model} to \emph{environment}, to clarify that it is not a model. Then discuss what if we do have a verified domain model. TODO}

\gregor{Add: The core question is then: How good is the model learned by the domain model learning algorithm?}
Many prior works evaluated their domain model learning algorithms by comparing the learned domain model to a \emph{reference domain model} (or \emph{ground truth model}). 
The reference domain model is assumed to be correct\gregor{i.e.\ it correctly models $E/\realm$}, and has been used to evaluate the learned domain model in terms of its \emph{syntactic similarity} to the reference model~\citep{aineto2019learning,mordoch2023safe,xi2024neuro,Oswald2024DLLMDomainModeling}.\roni{Others who have used, please add citations}
These syntactic similarity metrics typically compare the intersection or difference of the predicates in the actions' preconditions and effects between the learned and reference domain models. We define these common metrics below. 
Let $M$ and $\realm$ be the evaluated and reference domain models, respectively, and let $a$ be an action. We denote by $\pre_M(a)$ the preconditions of action $a$ according to domain $M$.
% Similarly, we denote by $L_a$ the set of all parameter-bound literals associated with action~$a$.[roni: I don't think this is a good fit for here, following CM's comment]

\cm{"parameter-bound literals" needs to be defined. Also, at this point, it's not clear what the model learner is given. Are we using the same types? Same objects? Same action signatures? What about same fluent names with different number of parameters? I guess all these questions are why we need new metrics beyond the syntactic ones, but some of the assumptions that go into using these (syntactic) metrics would be useful to stipulate.}
\roni{Hmm. You're right there's some blunder in the text above. I removed ``parameter-bound literal'' altogether and stayed with simply literals.}

% Let $M$ and $\realm$ be the evaluated and real domains, respectively, and let $a$ be an action. We denote by $\pre_M(a)$ and $\eff_M(a)$ the preconditions and effects, respectively, of action $a$ according to domain $M$.
 \begin{itemize}
    \item True Positives: TP$_\pre(a)=|(\pre_M(a)\cap \pre_\realm(a)|$
    \item False Positives: FP$_\pre(a)=|((\pre_M(a)\setminus \pre_\realm(a)|$
    \item True Negatives: TN$_\pre(a)=|L \setminus(\pre_M(a)\cup \pre_\realm(a))|$
    % , where $L_a$ is the set of all parameter-bound literals for an action $a$.
    \item False Negatives: FN$_\pre(a)=|(\pre_\realm(a)\setminus \pre_M(a))|$
\end{itemize}
The following standard metrics from statistical analysis can then be computed based on these values for each action:
\begin{itemize}
    \item \textbf{Syntactic Precision}: $P_\pre(a)=\frac{TP(a)}{TP(a)+FP(a)}$
    \item \textbf{Syntactic Recall}: $R_\pre(a)\frac{TP(a)}{TP(a)+FN(a)}$
\end{itemize}
Other metrics, such as Accuracy and F1-score, can also be computed based on these values. 
To obtain an overall precision and recall for preconditions of the entire domain model, one can compute the average of the precision and recall values for all actions:
$P_\text{avg}=\frac{1}{|A|}\sum_{a\in A} P(a)$ and $R_\text{avg}=\frac{1}{|A|}\sum_{a\in A} R(a)$, where $P(a)$ and $R(a)$ are the precision and recall of action $a$, respectively. 
The same metrics can be defined for the effects of actions, with the only difference being that the literals in the effects are used instead of those in the preconditions. 

\roni{TODO: Add an example}
\gregor{Proposed this ``pathological'' example}
As an example, consider a delivery scenario.
The true model $\realm$ has predicates $at$, $in$, and $contains$ to describe that a truck or package is at a location, a package is in a truck, and a truck contains a package.
The action unload has three parameters $\ell, t,$ and $p$ (location, truck, and package).
In $\realm$ its preconditions are $at(\ell, t)$ and $in(p,t)$, while its effects are $\neg in(p,t), \neg contains(t,p),$ and $at(p,t)$.
A learned model could conceivably have the same effects, but the preconditions $at(\ell, t)$ and $contains(t,p)$.
It would have syntactic recall and precision for the effects of $1$, but for the preconditions it would be $\frac 1 2$.

% As discussed above, while the syntactic similarity metrics are easy to compute, they do not accurately reflect functional differences between learned and original models. In addition, these metrics are not well-defined when the learned model and the original model have different action signatures for actions, predicates, or object types. Next, we present our domain model evaluation paradigm, which includes a set of metrics that measure model predictiveness and effectiveness and overcome representation differences between the learned domain model and the real domain model. 

%\mauro{add here the notion of strong/weak equivalence, and the metrics based on graph similarity}

In a slightly different fashion, \cite{chrpa2023comparing} proposed an approach to assess the edit distance of the learned domain model with regard to the reference one. Low distance values indicate models that are syntactically close to each other, and if two models are syntactically identical (i.e., edit distance of zero), then they are said to be \textit{strongly equivalent} \citep{chrpa2023comparing}.

%Similarly, the work done on model reconciliation \citep{} looks into approaches to suggest minimal changes to a model to reconcile it with a reference one.


%\roni{Add: discuss the limitation of the syntactic similarity metrics and the reliance on a reference model.}
The above approach to evaluating domain models has several limitations. 
First, it relies on the existence of a reference domain model, which may not be available in practice. 
In fact, this is usually the case in new applications of automated planning to real-world problems. In such cases, there may be a number of alternative models, but not a specific reference one. This is the setting, for instance, of the International Competition on Knowledge Engineering for Planning and Scheduling \citep{DBLP:journals/aim/ChrpaMVV17}.
\gregor{One could distinguish two cases here: (1) the practical application case, i.e. where we want to use domain learning in a real environment/industrial setting. Or (2) where we only want to evaluate the methods w.r.t.\ their usability. In the latter case, we could argue that we have ``exhaustively'' studied the setting we want to evaluate the model learner on and could thus have a planning model that models the situation -- but still in this case, the issue of multiple equivalent models remains.}


\gregor{Maybe add a new second reason here to prepare the argumentation for the next one:\\
Second, there might be multiple models that are fully identical for all practical purposes.
That is, we will be able to find exactly the same set of plans using all of these models.
A supposed ground truth model would then be an arbitrary pick out of these equivalent models.
In evaluation would reward a learner if it can reproduce the one arbitrarily chosen ground truth model over any other equivalent model.
Considering the above transport example, it is equivalent to use $in(p,t)$ or $contains(t,p)$ as a precondition -- but using syntactic similarities forces the learner to have a bias for one of the two option, without being able to infer which bias is correct out of the input trajectories.
}

Second, the reference model is assumed to be of the best possible quality. This is in itself complicated by the fact that it is very challenging to assess the quality of a model \citep{DBLP:conf/kcap/McCluskeyVV17}. Besides this, the use of a reference model biases the assessment towards a single specific encoding -- while many others of similar quality could potentially exist.
Third, it is not clear that the syntactic similarity of the learned model to a reference model is a good indicator of how \emph{useful} a learned domain model is. This limitation has been observed in prior works~\citep{aineto2019learning,juba2021safe,mordoch2024safe}.
Next, we discuss what constitutes a useful domain model and how to evaluate it without the need for a reference domain model.




\section{Metrics for Evaluating Domain Models}

We can consider two main dimensions to optimize the learning process and to evaluate the learned model:
%
%In this setup, we consider the following desired objectives of a learned domain model. 
\begin{itemize}
    % \item \textbf{Syntactic similarity.} Aim to learn a domain model that is  as similar as possible syntactically to the reference one. 
    \item \textbf{Predictive power.} Aim to learn a domain model that allows predicting the outcome \gregor{and applicability?} of actions in the environment. 
    \item \textbf{Problem-solving ability.} Aim to learn a domain model that enables the generation of solutions within given reasonable resource bounds. %enables solving problems in the real domain model.
    \gregor{Question here is: which solutions? (1) A plan that solves the learned problem? Definitely not, as this is not useful in the real environment. (2) A plan that will be executable in the real environment!}
\end{itemize}

%\mauro{reshape discussion below -- align problems-solving ability to operationality notion.}

A key insight to consider 
%While the real domain model satisfies all three objectives, a key insight 
is that these dimensions are not necessarily aligned with the syntactic similarity metrics defined above, even if a reference model exists that enables computing it.  
A domain model may be very \emph{predictive} of 
behavior of the agent in the reference domain model, yet very different from a reference model since it encodes mutex conditions that the reference model did not bother to define. \roni{Is this clear?}\yarin{Yes but, how is this instead: A domain model can be highly \emph{predictive} of an agent's behavior in the reference domain, even if it differs significantly from the reference model. This can happen when the learned model encodes mutex (mutual exclusion) conditions that the reference model does not explicitly define.
}
\gregor{maybe: ``does only define implicitly''?} \gregor{I've also put this issue already in the example above. Is this more illustrative?}
Similarly, a learned domain model may be very \emph{similar syntactically} to a reference domain model but not effective in solving problems from the corresponding real-world environment, 
e.g., adding redundant preconditions or missing a crucial effect \citep{DBLP:conf/kcap/VallatiC19}. \gregor{or missing a crucial precondition: e.g.\ can only open a door if I have key.}
In contrast, a domain model may be very different from a reference domain model, e.g., adding many redundant preconditions to some actions, yet very \emph{effective} in solving problems in the application domain since these redundant preconditions are often true in the real world. 


The two dimensions described here --- predictive power and problem-solving ability -- are also not necessarily aligned. Indeed, a domain model may be very \emph{predictive} of behavior of the agent in the reference domain model, yet too complex to be used for solving problems by any existing planning engine.
Assume, for example, that a learned domain model that has many copies of the same action in the reference domain model, each with different parameters and preconditions, in order to capture different aspects of that action's behavior. 
While this may be useful for predicting the applicability of actions in the reference domain model, 
it may hinder the ability of a planning engine to find plans for problems in the application domain with this model, due to its complexity (e.g., large branching factor). 
Therefore, different metrics are required to evaluate these two dimensions. We describe such metrics below. 
% \roni{I think this can be merged, talking immediately on the case where there is no reference model for these metrics. Not sure about this. Thoughts?}
% We begin by assuming that the learned domain model and the reference domain model both use the same representation of states and actions, %i.e., they are \emph{syntactically similar} 
% in the sense that they have the same action names, object types, and parameter ordering. This is indeed generally the case for approaches that learn models from trajectories and plan traces, that come with a pre-defined vocabulary. 
% In Section~\ref{sec:bridging-gap}, we discuss how to extend the metrics to handle cases where the learned domain model and the reference domain model use different representations. 



\subsection{Predictive Power Metrics}
\label{sec:predictiveness-metrics}
While the syntactic similarity metrics are easy to compute, they do not accurately reflect the \emph{predictive power} of the learned model. 
%The \emph{predictive power} metrics described next are designed to address this issue. 
Predictive power metrics, referred to sometimes as \emph{semantic} domain model metrics~\citep{aineto2019learning,mordoch2024safe,le2024learning}, are based on the idea that a learned model should be able to predict the applicability of actions and their effects in the environment. 

\cm{Up until this point, I thought it was headed towards the predictive power of explaining the trajectory data. Given that it's about action applicability, this needs to be clarified much earlier on (e.g., intro). Also means that we don't have a notion of how well the learned model predicts the trajectories?}

We define two types of predictive power metrics: \emph{action applicability} metrics and \emph{predicted effects} metrics. 
The former measures the ability of the learned model to predict whether an action is applicable in a given state, while the latter measures the ability of the learned model to predict the effects of an action in a given state.
% These metrics require a dataset of (state, action) pairs 
Unlike the syntactic similarity metrics, which are computed based on the action model itself, the predictive power metrics require a dataset of states that we denote by $S$. 
This dataset is intended to represent the distribution of states of interest in the domain. 
One way to create this dataset is by running a planner on a set of test problems with the real action model.\gregor{Or to actually observe states that appear in the actual environment during execution.}

\cm{We still need the original domain in order to compute the predictive power. Some of the criticisms lofted at the existing metrics may need to be scaled back because of this -- there's no way to know if the predicted actions that are applicable are the right ones, without having a reference model.}

\roni{TODO: Clarify this does not consider complexity of the domain (which will be addressed in the next metric)}

\gregor{Issue: what happens if we either cannot get the states themselves, i.e., no symbolic description or if the state description of the actual model differs from the one of the learned model? (say: predicates are named differently?)\\
Can we be more radical here? Maybe it is sufficient to have plans and non-plans for the real model? Then we could check whether these plans are executable in the learned model and whether the non-plans are not. One issue: this mixes applicability and effects.
}

\cm{Ya, I think many of the definitions start to fall apart if we don't have several assumptions down -- same objects, same types, same fluent/action symbols with their signatures, etc.}

\paragraph{Predicted applicability}
For a domain model $M$ and action $a$, we denote by $\app_M(a,S)$ the set of states in $S$ in which $a$ is applicable according to $M$. \gregor{This is easy to compute for STRIPS/SAS+, but hard if you have things like disjunctive preconditions. It should be \#P-hard}
\roni{Hmm. I don't see what this is not linear in the size of $S$ and the preconditions of $a$ in $M$: we iterate over every state in $S$ and then check whether $a$ is applicable in it according to $M$. I guess if we have some universals in the preconditions this might be harder?}
Based on this notation, we define the following predicted action applicability metric as follows for some action $a$:
\begin{itemize}
    \item TP$_{\app}(a)=|\app_M(a,S)\cap \app_\realm(a,S)|$
    \item FP$_{\app}(a)=|\app_M(a,S)\setminus \app_\realm(a,S)|$ 
    \item TN$_{\app}(a)=|S\setminus (\app_M(a,S)\cup \app_\realm(a,S))|$
    \item FN$_{\app}(a)=|\app_\realm(a,S)\setminus \app_M(a,S)|$
\end{itemize}
In words, TP$_{\app}(a)$ is the number of states in $S$ where $a$ is applicable according to both the learned model and the real model, FP$_{\app}(a)$ is the number of states in $S$ where $a$ is applicable according to the learned model but not the real model, TN$_{\app}(a)$ is the number of states in $S$ where $a$ is not applicable according to both models, and FN$_{\app}(a)$ is the number of states in $S$ where $a$ is applicable according to the real model but not the learned model. 
From these metrics, one can compute the precision and recall of the learned model for action applicability as follows,
\begin{align}
    P_{\app}(a)= & \frac{TP_{\app}(a)}{TP_{\app}(a)+FP_{\app}(a)}\\
    R_{\app}(a)= & \frac{TP_{\app}(a)}{TP_{\app}(a)+FN_{\app}(a)}    
\end{align} 
and the overall action applicability, precision, and recall
by averaging over all actions.  

\paragraph{Predicted effects}
For domain $M$, action $a$, and state $s$, we denote by $a_M(s)$ the state resulting from applying $a$ in $s$ according to $M$. 
Based on this, we define the following predicted action effect metrics for some action $a$ and state $s$, as follows: 
\begin{itemize}
    \item TP$_{\eff}(s,a)=|(a_M(s)\setminus s)\cap (a_\realm(s)\setminus s)|$
    \item FP$_{\eff}(s,a)=|(a_M(s)\setminus s)\setminus a_\realm(s)|$ 
    % \yarin{I think it should be $|(a_M(s)\setminus s)\setminus (a_\realm(s)\setminus s)|$ (effect minus effect and not effect minus state) and if so also FN accordingly}\gregor{For all sets it holds that $(X \setminus Y) \setminus Z = X \setminus (Y \cup Z) = X \setminus (Y \cup Z) = X \setminus (Y  \cup (Z\setminus Y)) = (X \setminus Y) \setminus (Z\setminus Y)$. In short: removing $s$ twice does not make a difference. Both formulations are the same.}\yarin{Thank you for the clarification}
    \item TN$_{\eff}(s,a)=|s \cap a_M(s) \cap a_\realm(s)|$
    \item FN$_{\eff}(s,a)=|(a_M(s)\cap s)\setminus a_\realm(s)|$ \gregor{there is a start too many (this is always $\emptyset$) I assume it should be $(a_M(s)\cap s)\setminus (a_\realm(s) \cap s)$ -- the facts that are not changed by $a_M$, but not counting the ones changes by $M_\realm$}\roni{Good catch! I fixed it in a slightly different way, let me know if you agree or not.}\cm{The TN and FN aren't looking correct...but after spending some minutes with it, I think it is ;). Should state that a "negative" here indicates a literal that remains unchanged.}
\end{itemize}
For the purpose of the above computation, a state includes all the literals true in it, i.e., both positive propositions and negative ones. 
% \roni{Ensure this makes sense} \gregor{Checked.}TNX!
One can aggregate the above metrics over all states and actions, \gregor{This might be computationally a problem -- so we sample?!}\roni{Hmm. I am not sure. Is it worse than low-order polynomial in $S$ and number of actions?}
compute the overall values, and compute aggregated precision and recall values accordingly.  
The difference between the syntactic similarity of effects and the predicted effects metrics is subtle. 
It manifests when processing observations in which some literal $\ell$ is observed in the state before an action $a$, 
it is not an \gregor{posisitve i.e.\ adding?} effect according to the learned model, but it is an effect according to the real model. 
This will count as a false positive in the syntactic similarity metrics yet not count as a false positive in the predicted effects metrics. 

\roni{Add a concrete example of how to compute these metrics}
\yarin{I can add an example after we verify the different formulas, example for each or only for Predicted effects ?}

 
\subsection{Problem-Solving Metrics}
%\mauro{add here disclaimer}\roni{Added the disclaimer below. Let me know what you think}
Neither the syntactic similarity metrics nor the predictive power metrics are sufficient for evaluating the operationality \citep{DBLP:conf/kcap/McCluskeyVV17} of the learned domain model, i.e. its ability to solve problems. It is well-known that even small syntactical changes in domain models can result in significant performance gaps \citep{DBLP:conf/kcap/VallatiC19,vallati2021importance}.

%effectiveness of using a learned domain model to actually solve problems in the application domain. 
We propose two metrics that are designed to address this issue: \emph{solving ratio} and \emph{false plans ratio}. 
Both metrics are defined with respect to a set of problems $P$ in the environment $\realm$. 
The solving ratio metric is defined as the ratio of problems in $P$ that can be solved with the learned model by a given planning within a fixed limit on the available computational resources --- CPU runtime and memory. 
We say that a problem is solved if a plan is found within the limited computational resources, and that plan is valid according to the reference action model. 
The false plans ratio metric is defined as the ratio of problems in $P$ that are solved by the learned model but are not valid according to the real action model. This reflects the reliability of using plans with the learned model.

% Limitations
The straightforward nature of the above metrics is not without limitations. The ability to solve a problem with a given domain depends on external factors such as the set of test problems, the planner used, the runtime and memory budget allowed for it to run, and the computer and OS that executed the planner. Ideally, the above metrics would be run on a diverse set of test problems, planners, resource limits, and computing machines. In practice, this might be difficult to implement, but one is advised to at least run all evaluated domains on the same setup and provide an appropriate disclaimer to the concluded result. 

\roni{TODO: Add metric on the runtime of solving the problem.}
\roni{TODO: Discussion: what about a metric on how many real plans can be validated by the learned model.}\mauro{Yes! validation capability of the learned model with regards to plans generated with the reference model!}
\roni{TODO: Maybe: add some unsolvability detection metrics?}
\cm{In our work on aligning (which is very closely related, it seems!), we computed both (1) if the plans found using P and M validate on M* and (2) if the plans found using P and M* validate on M. You need assumptions on the action names+parameters, but then can test (via VAL) both ways.}



%All the metrics above are defined based on the assumption that the ground truth model is available. 
%However, in many cases, the ground truth model is not available. 
%Moreover, the ground truth model may be a suboptimal representation for the actual environment. 

Note that all the metrics described in this section do not require a reference model, but instead are with respect to the actual environment. Thus, they can be used to compare two models directly,  providing statistics on which one is better according to different aspects of the environment.
\cm{I think the above is misleading. We need to have states (fluents matching the learned model), action applicability (for predictive power), etc, etc. We do need a reference model! What's changed is that we aren't using syntactic comparisons anymore, but the M* model is certainly still there.}
% For the predictive power metrics, we can use the same dataset of states $S$ to compute the predicted applicability and predicted effects metrics for each evaluated model. Similarly, the problem-solving metrics can be computed for each model without a reference model, if one can execute the plans generated by the learned model in the real environment, then we can evaluate the solving ratio and false plans ratio metrics. 


%If two models are generated from scratch and compared



% particularly since certain predicates or negative conditions in one model might implicitly imply others, complicating direct comparisons and similarly there exists domains (rovers for example) that rely on add-delete of effects mechanism where it adds and deletes the same literal $l$ in the same transition to simulate a continuous domain but it cannot be represented in the trajectory used as an input for the learning algorithms. Furthermore, another limitation arises when learned domain actions or predicates do not share identical signatures with those of the original domain, making direct syntactic comparisons even more challenging.


\section{Bridging Representation Gaps}
\label{sec:bridging-gap}
% Motivation: representation may be different learning
Some domain model learning algorithms output a classical planning domain model that uses a different representation. Such a model can still be used by a planner, yet its input and output may be incompatible with that of the reference domain model. 
In some cases, these representation differences are minimal, e.g., using different ordering of the parameters or object types. 
In other cases, the differences are more significant. 
For example, the ESAM algorithm~\citep{juba2021safe} outputs a domain model with additional \emph{proxy actions} that are used to resolve ambiguities in the mapping of objects to parameters.
The proxy actions are not part of the original domain model, and they are not used in the reference domain model. 
\roni{More examples?}

\mauro{OpMaker2 generates classical planning models in OCL language \cite{mccluskey2010action}}

\yarin{added LOCM:}
Similarly, the LOCM algorithm~\citep{cresswell2013acquiring} induces finite-state machines solely from action sequences-with no access to intermediate state information or predicate names-and outputs action schemas using \emph{proxy predicates}. 
% (e.g., \texttt{object\_state0}, \texttt{object\_state1(agent)}). 
These proxy predicates are not part of the original domain but are introduced as a mechanism to infer the semantics of the original predicates. 
% For example, a learned transition from \texttt{object\_state1} to \texttt{object\_state0} may represent an object being put down, corresponding to a change in the original predicate \texttt{holding(agent, object)}.


The syntactic similarity metrics are not relevant in such cases, where the representation is intentionally different from the reference model. Next, we describe how to apply the other metrics -- predictive power and problem-solving -- in such cases. 
Note that we limit the discussion to the case where the learned domain is still a planning domain, as opposed to images or other more involved representations. 

\subsection{The Encoder-Decoder Mechanism}
Let $R_{\realm}$ denote the input representation and let $R_M$ denote the representation of the learned domain model. 
The given trajectories are given in $R_{\realm}$, 
while the output of a planner that uses the learned domain model is in $R_M$. 
To allow the comparison of the learned domain model with the reference domain model, we need to bridge the gap between these two representations. 
To this end, we require every domain model learning algorithm to output 
a representation \emph{encoder} and a representation \emph{decoder} in addition to the learned model. We describe these two components below.  
The \emph{encoder} transforms states and actions in $R_{\realm}$ to corresponding states and actions in the learned domain representation $R_M$. 
The \emph{decoder} performs the reverse transformation, mapping states and actions in $R_M$ to corresponding states and actions in $R_\realm$.


Formally, let $A$ and $A_M$ be the set of actions in the real and learned domain models, respectively, and let $S$ and $S_M$ be the set of states in the real and learned domain models, respectively. The power set is denoted by $P(X)$, which is the set of all subsets of $X$. 
The encoder is required to implement the following set of functions:
\begin{itemize}
    \item ${\mathit{encodeAction}: S\times A\rightarrow P(A_M)}$, returns the set of actions in $M$ that represent the application of $a$\yarin{$a \in A$} in a given state $s$\yarin{$s \in S$}. 
    \item ${\mathit{encodeState}: S\rightarrow S_M}$, returns the state in $M$ that represents the state $s$ in $\realm$. \yarin{returns the state $s \in S$ as its representation in $S_M$}
\end{itemize}
The decoder is required to implement the following functions:
\begin{itemize}
    \item ${\mathit{decodeAction}: A_M\rightarrow A}$, returns the action in $\realm$ corresponding to the action in $M$. \yarin{returns the action $a \in A_M$ as its representation in $A$}
    \item ${\mathit{decodeState}: S_M\rightarrow S}$, returns the state in the input representation ($S$) that represents a given state in $S_M$. \yarin{returns the state $s \in S_M$ as its representation in $S$}
\end{itemize}





% \end{itemize}
    
%     are a proxy of the action $a$.
    
%     $\text{ and} \\ encodeAction(s,a,m^*,m)$=
% ${\{a'\in m| a'\ is\ a\ proxy\ action\ of\ a\}}$
%     \item ${\mathit{decodeAction}: A_{m}\rightarrow  A_{m^*} }$ $\text{ and } {\mathit{decodeAction}(s,a',m^*,m)}$$  a^*\in A_{m^*}$ s.t. $a^*$ is a proxy of $a$. 



% \begin{itemize}
%     \item $encodePlan: \{\pi_{m^*}\in \pi(A_{m^*})\}\rightarrow P(\pi_{m}\in \pi(A_{m}))$
%     \item $\mathit{decodePlan}:\{\pi_{m}\in \pi(A_{m})\} \rightarrow  \{\pi_{m^*}\in \pi(A_{m^*})\}$
% \end{itemize}

% \begin{itemize}
%     \item $encodePredicates:P( P_{m^*}) \rightarrow  P(P_m) \\$  and $encode{Predicates}(P'\subseteq P^*,m^*,m) = P''\subseteq P_m$
%     \item $\mathit{decodePredicates}: P( P_{m}) \rightarrow  P(P_{m^*}) $  and $\mathit{decodePredicates}(P'\subseteq P_{m^*},m^*,m) = P''\subseteq P_m$. 
% \end{itemize}

\subsection{Predictive Power Metrics with an Encoder-Decoder}
For the predicted applicability metric, we modify the way $\app_M(a,S)$ is computed. The main difference is that a single action in the reference domain model may be represented by multiple actions in the learned model. 
Thus, we define 
$\app_M(a,S)$ to be the number of states in $S$ in which \emph{there exists} an action $a'$ in the learned model such that $a'$ is applicable in the state in $M$ that encodes $s$. 

\roni{TODO: Add a formal definition of the $app_M$ method using the encoder and decoder methods}

Similarly, for the predicted effects metric, we need to decode the state resulting from $a_M(s)$ instead of using it as-is. 

\roni{TODO: Add a formal definition of the $app_M$ method using the encoder and decoder methods}

\subsection{Problem-Solving Metrics with an Encoder-Decoder}

Adapting the problem-solving metrics to use the encoder-decoder mechanism is straightforward.
The encoder is used to encode the problem from the input representation to the learned model representation. 
Then, the planner is run on the encoded problem with the learned model. 
Finally, the decoder is used to decode the solution plan from the learned model representation to the input representation. 
This allows checking if a plan has been found and if it is valid according to the reference domain model. 


\subsection{Case Studies}

Next, we describe how the encoder-decoder mechanism can be implemented for several action model learning algorithms. \yarin{Some action model learning algorithms operate directly in the original input representation $R_{\realm}$, without transforming the state or action spaces. For these algorithms, the learned model uses the same symbols and structure as the input data. As such, there is no need for additional encoding or decoding: the identity function suffices for both.}

SAM~\citep{stern2017efficient}\yarin{or is ut juba2021safe ?}: \roni{Either me or someone from my group will write this one}
Encoder: identity
Decoder: identity

FAMA~\citep{aineto2019learning} - Leonardo L.?:
Encoder: identity
Decoder: identity - maybe change to parameter names?

ESAM~\citep{juba2021safe}: \roni{Either me or someone from my group will write this one}
Encoder: identity for both state and actions     
Decoder: translate proxy actions to original actions

ROSAME-I~\citep{xi2024neuro}: \roni{Maybe Yarin or Argaman can help with this one}
Encoder: encode to a numeric vector of ones and zeros
Decoder: map action parameters

OBSERVER\yarin{add cite}: \roni{Anyone can help here?}
Encoder: from binding to grounded
Decoder: from grounded predicates and actions to the binding they represent

LOCM~\citep{cresswell2013acquiring}:\yarin{I will complete this}
Encoder: identity for action name and proxy for state predicates
Decoder: translate proxy predicates to original predicates

\yarin{Add NOLAM too?}
\roni{Most important: anyone can help here?}


\subsection{An Evaluation Paradigm}
Using the above metrics and the encoder-decoder mechanism, we can define an evaluation paradigm for action model learning algorithms.
The evaluation paradigm consists of the following steps: 
First, we generate a set of trajectories in the input representation for learning. 
This set of trajectories can be generated by running a planner on a set of problems in the reference domain model or via a random walk.
Then, this set of trajectories is split in 80/20 ratio, using 80\% of the trajectories for training and 20\% for testing. 
The evaluated learning algorithm is given the training set of trajectories and is required to output a learned domain model, an encoder, and a decoder. 
Following we use the learned model, encoder, and decoder to compute the metrics described above. 
This 80/20 split is repeated $k$ times following a standard $k$-fold validation process. 

\cm{An eval-heavy ICAPS'er may take issue with some of the suggestions (random walks are hard to do right, the benchmark problem sets aren't IID, etc, etc. I'd say we're probably mostly fine (someone can always find some issue if they try hard enough), but we \textit{should} comment on the disconnect of trace -vs- state. Many approaches require traces, but the evals require state sets. Taking the latter to just be states along traces is a deliberate choice that warrants discussion.}

% \begin{algorithm}[h]
% \caption{Simple Increment Function}
% \begin{algorithmic}[1]
% \REQUIRE Integer $x$
% \ENSURE Integer $y = x + 1$
% \FUNCTION{Increment}{$x$}
%     \STATE $y \leftarrow x + 1$
%     \RETURN $y$
% \ENDFUNCTION
% \end{algorithmic}
% \end{algorithm}


\section{Benchmarks}

\todo{Leonardo is in charge of this section.}

\todo{Describe the benchmark suite and how to use it.}

\todo{If we have time: show results for at least some of the algorithms on the benchmark suite.}

\section{Discussion}
\roni{For all: please read and let me know what you think by adding comments in the text (e.g., add text like this ``[[YourName: bla bla bla'']]) or editting.}
% Different input representations
The proposed evaluation paradigm is designed to be flexible and extensible. 
In particular, it can be extended to support other types of representations, such as images or other types of data. 
Another possible extension is to allow the action model learning algorithm to output a domain model that uses a more general type of planning. 
For example, the action model learning algorithm may output a domain model that uses a probabilistic~\citep{xi2024neuro} or partially observable representation~\citep{le2024learning}. 
Adapting the predictive power metrics to such cases may not be trivial, but the problem-solving metrics can be adapted in a straightforward manner.


% Beyond discrete
Going beyond discrete representations is also possible. 
The predicted applicability metrics can be computed as-is, considering both numeric and discrete preconditions.
More involved is the predicted effects metric, which may require defining some loss function for the numeric effects, as learning exactly the same numeric effects as in the reference domain model seems unlikely. 
Note that in all of these variants and generalizations, the encoder-decoder mechanism allows seamless use of the problem-solving metrics. 


% Online metrics 
\todo{Maybe talk about metrics for online learning}
The metrics and evaluation paradigm outlined in this paper are for \emph{offline} learning of action models, where the learning algorithm is given a set of trajectories and is required to output a domain model. \emph{Online learning} of action models have also been studied in the literature~\citep{lamanna2021online, sreedharan2023optimistic, benyamin2025integratingreinforcementlearningaction, ng2019incremental, chitnis2021glib, verma2023autonomous, karia2023epistemic, jin2022creativity}\todo{More citations?}\yarin{added, to many?}, where the learning algorithm is required to learn a domain model by actively interacting with the environment. Similar to other online tasks, one may distinguish between the \emph{cumulative regret} of the learning process, which can be the number of actions performed for learning or the number of problems failed to solve while collecting observations. 
Specific metrics and evaluation for online learning of action models, however, is beyond the scope of this work. 

Finally, models can also be compared not only in terms of their characteristics, but also in terms of the processes used to learn or generate them \citep{vallati2021quality}, a perspective that is commonly used for assessing conceptual models.



\subsection{Relation to Model Reconciliation}
\roni{Pascal and Mauro: maybe put here is a discussion on what is model reconciliation and how it relates to our work, 
and existing model reconciliation metrics?}

\subsection{Relation to Model Repair}
\roni{Pascal and Mauro: same for model repair?}

\section{Conclusion Future Work}
\roni{I'll fix this text later}
% TODO VERIFY TEXT BELOW
We presented a new paradigm for evaluating action 
model learning algorithms across different representations. In this paradigm, each action model learning algorithm is required to output an action model and an encoder-decoder pair. The encoder-decoder pair is used to bridge representation gaps and enable measuring the problem-solving capabilities of the learned action models. We proposed an evaluation scheme that leverages the encoder-decoder pair to systematically compare learned action models and described several evaluation metrics. A benchmark suite was also provided to facilitate the evaluation of action model learning algorithms, based on existing domain models from the International Planning Competition (IPC). We demonstrated our evaluation paradigm by applying it to several action model learning algorithms, including SAM, ESAM, FAMA, and ROSAME.



\bibliographystyle{kr}
\bibliography{library} 


\end{document}

